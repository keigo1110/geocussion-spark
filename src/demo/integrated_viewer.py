#!/usr/bin/env python3
"""
çµ±åˆãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼ï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç† + UIè¡¨ç¤ºï¼‰
è²¬å‹™: HandledPipelineã¨çµ„ã¿åˆã‚ã›ãŸçµ±åˆè¡¨ç¤ºã‚·ã‚¹ãƒ†ãƒ 
"""

import time
import threading
from typing import Optional, List, Dict, Any
import numpy as np
import cv2

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†
from .pipeline_wrapper import HandledPipeline, HandledPipelineConfig, PipelineResults

# UIè¡¨ç¤ºï¼ˆOpen3Dï¼‰
try:
    import open3d as o3d
    HAS_OPEN3D = True
except ImportError:
    HAS_OPEN3D = False
    o3d = None  # type: ignore

# éŸ³éŸ¿è¨­å®š
from ..sound.mapping import ScaleType, InstrumentType
from ..input.stream import OrbbecCamera
from ..types import FrameData, OBFormat, CameraIntrinsics

# ã‚¤ãƒ™ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
from .events import (
    get_event_dispatcher,
    EventHandler,
    EventType,
    KeyPressedEvent,
    WindowResizedEvent,
    ViewportChangedEvent,
    FrameProcessedEvent,
    MeshUpdatedEvent,
    CollisionDetectedEvent,
    StageCompletedEvent,
    ErrorEvent
)
from .events.pipeline_events import MeshUpdatedEvent as MeshEvent
from .events.pipeline_events import CollisionDetectedEvent as CollisionEvent
from .events.config_handler import ConfigurationEventHandler


class IntegratedGeocussionViewer(EventHandler):
    """
    çµ±åˆGeocussionãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼
    HandledPipelineã¨UIè¡¨ç¤ºã‚’çµ„ã¿åˆã‚ã›ãŸçµ±åˆã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self, config: Optional[HandledPipelineConfig] = None, **kwargs: Any) -> None:
        """
        åˆæœŸåŒ–
        
        Args:
            config: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®šï¼ˆæŒ‡å®šã•ã‚Œãªã„å ´åˆã¯kwargsã‹ã‚‰æ§‹ç¯‰ï¼‰
            **kwargs: è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆconfigæœªæŒ‡å®šæ™‚ã«ä½¿ç”¨ï¼‰
        """
        print("çµ±åˆãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼åˆæœŸåŒ–ä¸­...")
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®šæ§‹ç¯‰
        if config is not None:
            self.pipeline_config = config
        else:
            self.pipeline_config = HandledPipelineConfig(
                enable_filter=kwargs.get('enable_filter', True),
                enable_hand_detection=kwargs.get('enable_hand_detection', True),
                enable_tracking=kwargs.get('enable_tracking', True),
                min_detection_confidence=kwargs.get('min_detection_confidence', 0.7),
                use_gpu_mediapipe=kwargs.get('use_gpu_mediapipe', False),
                
                enable_mesh_generation=kwargs.get('enable_mesh_generation', True),
                mesh_update_interval=kwargs.get('mesh_update_interval', 10),
                max_mesh_skip_frames=kwargs.get('max_mesh_skip_frames', 60),
                mesh_resolution=kwargs.get('mesh_resolution', 0.01),
                mesh_quality_threshold=kwargs.get('mesh_quality_threshold', 0.3),
                mesh_reduction=kwargs.get('mesh_reduction', 0.7),
                
                enable_collision_detection=kwargs.get('enable_collision_detection', True),
                enable_collision_visualization=kwargs.get('enable_collision_visualization', True),
                sphere_radius=kwargs.get('sphere_radius', 0.05),
                
                enable_audio_synthesis=kwargs.get('enable_audio_synthesis', True),
                audio_scale=kwargs.get('audio_scale', ScaleType.PENTATONIC),
                audio_instrument=kwargs.get('audio_instrument', InstrumentType.MARIMBA),
                audio_polyphony=kwargs.get('audio_polyphony', 16),
                audio_master_volume=kwargs.get('audio_master_volume', 0.7),
                
                enable_voxel_downsampling=kwargs.get('enable_voxel_downsampling', True),
                voxel_size=kwargs.get('voxel_size', 0.005),
                enable_gpu_acceleration=kwargs.get('enable_gpu_acceleration', True)
            )
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        self.pipeline = HandledPipeline(self.pipeline_config)
        
        # ã‚«ãƒ¡ãƒ©è¨­å®š
        self.camera: Optional[OrbbecCamera] = None
        self.depth_width = kwargs.get('depth_width')
        self.depth_height = kwargs.get('depth_height')
        
        # UIè¨­å®š
        self.rgb_window_size = kwargs.get('rgb_window_size', (640, 480))
        self.point_size = kwargs.get('point_size', 2.0)
        
        # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰è¨­å®š
        self.headless_mode = kwargs.get('headless_mode', False)
        self.headless_duration = kwargs.get('headless_duration', 30)
        self.pure_headless_mode = kwargs.get('pure_headless_mode', False)
        
        # Open3Dé–¢é€£
        self.vis: Optional[o3d.visualization.Visualizer] = None
        self.pcd: Optional[o3d.geometry.PointCloud] = None
        
        # å¯è¦–åŒ–ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        self.mesh_geometries = []
        self.collision_geometries = []
        self.hand_markers = []
        
        # çŠ¶æ…‹ç®¡ç†
        self.is_running = False
        self.is_initialized = False
        self.current_results: Optional[PipelineResults] = None
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¡¨ç¤º
        self.show_performance = False
        
        # ãƒ˜ãƒ«ãƒ—ãƒ†ã‚­ã‚¹ãƒˆ
        self.help_text = self._build_help_text()
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.event_dispatcher = get_event_dispatcher()
        self._subscribe_to_events()
        
        # è¨­å®šå¤‰æ›´ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆæœŸåŒ–
        self.config_handler = ConfigurationEventHandler(self.pipeline_config, self.pipeline)
        self.event_dispatcher.subscribe(EventType.KEY_PRESSED, self.config_handler)
        
        print("çµ±åˆãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼åˆæœŸåŒ–å®Œäº†")
        print(f"  - ãƒ¡ãƒƒã‚·ãƒ¥ç”Ÿæˆ: {'æœ‰åŠ¹' if self.pipeline_config.enable_mesh_generation else 'ç„¡åŠ¹'}")
        print(f"  - è¡çªæ¤œå‡º: {'æœ‰åŠ¹' if self.pipeline_config.enable_collision_detection else 'ç„¡åŠ¹'}")
        print(f"  - éŸ³éŸ¿åˆæˆ: {'æœ‰åŠ¹' if self.pipeline_config.enable_audio_synthesis else 'ç„¡åŠ¹'}")
        print(f"  - ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if self.headless_mode else 'ç„¡åŠ¹'}")
    
    def _build_help_text(self) -> str:
        """ãƒ˜ãƒ«ãƒ—ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰"""
        help_text = "=== Geocussion çµ±åˆãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼ ===\n"
        help_text += "ESC/Q: çµ‚äº†\n"
        help_text += "H: ãƒ˜ãƒ«ãƒ—è¡¨ç¤º\n"
        help_text += "\n=== ãƒ¡ãƒƒã‚·ãƒ¥ãƒ»è¡çªåˆ¶å¾¡ ===\n"
        help_text += "M: ãƒ¡ãƒƒã‚·ãƒ¥ç”Ÿæˆ ON/OFF\n"
        help_text += "C: è¡çªæ¤œå‡º ON/OFF\n"
        help_text += "V: è¡çªå¯è¦–åŒ– ON/OFF\n"
        help_text += "N: ãƒ¡ãƒƒã‚·ãƒ¥å¼·åˆ¶æ›´æ–°\n"
        help_text += "+/-: çƒåŠå¾„èª¿æ•´\n"
        help_text += "P: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆè¡¨ç¤º\n"
        
        if self.pipeline_config.enable_audio_synthesis:
            help_text += "\n=== éŸ³éŸ¿åˆ¶å¾¡ ===\n"
            help_text += "A: éŸ³éŸ¿åˆæˆ ON/OFF\n"
            help_text += "S: éŸ³éšåˆ‡ã‚Šæ›¿ãˆ\n"
            help_text += "I: æ¥½å™¨åˆ‡ã‚Šæ›¿ãˆ\n"
            help_text += "1/2: éŸ³é‡èª¿æ•´\n"
        
        return help_text
    
    def _subscribe_to_events(self) -> None:
        """ã‚¤ãƒ™ãƒ³ãƒˆã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š"""
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆ
        self.event_dispatcher.subscribe(EventType.FRAME_PROCESSED, self)
        self.event_dispatcher.subscribe(EventType.MESH_UPDATED, self)
        self.event_dispatcher.subscribe(EventType.COLLISION_DETECTED, self)
        self.event_dispatcher.subscribe(EventType.STAGE_COMPLETED, self)
        self.event_dispatcher.subscribe(EventType.PIPELINE_ERROR, self)
    
    def handle_event(self, event) -> None:
        """
        ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè£…
        
        Args:
            event: å‡¦ç†ã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆ
        """
        try:
            if event.event_type == EventType.FRAME_PROCESSED:
                # ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†å®Œäº†ã‚¤ãƒ™ãƒ³ãƒˆ
                # ç¾åœ¨ã¯_process_frameã§ç›´æ¥å‡¦ç†ã—ã¦ã„ã‚‹ã®ã§ã€å¿…è¦ã«å¿œã˜ã¦å®Ÿè£…
                pass
                
            elif event.event_type == EventType.MESH_UPDATED:
                # ãƒ¡ãƒƒã‚·ãƒ¥æ›´æ–°ã‚¤ãƒ™ãƒ³ãƒˆ
                if self.vis and not self.headless_mode:
                    # ãƒ¡ãƒƒã‚·ãƒ¥å¯è¦–åŒ–ã‚’æ›´æ–°
                    self._update_mesh_from_event(event)
                    
            elif event.event_type == EventType.COLLISION_DETECTED:
                # è¡çªæ¤œå‡ºã‚¤ãƒ™ãƒ³ãƒˆ
                if self.vis and not self.headless_mode:
                    # è¡çªå¯è¦–åŒ–ã‚’æ›´æ–°
                    self._update_collision_from_event(event)
                    
            elif event.event_type == EventType.STAGE_COMPLETED:
                # ã‚¹ãƒ†ãƒ¼ã‚¸å®Œäº†ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆãªã©ï¼‰
                if self.show_performance:
                    print(f"Stage {event.stage_name} completed in {event.processing_time_ms:.1f}ms")
                    
            elif event.event_type == EventType.PIPELINE_ERROR:
                # ã‚¨ãƒ©ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆ
                print(f"Pipeline error in {event.stage_name}: {event.error_message}")
                
        except Exception as e:
            print(f"Event handling error: {e}")
    
    def _update_mesh_from_event(self, event: MeshEvent) -> None:
        """ãƒ¡ãƒƒã‚·ãƒ¥æ›´æ–°ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰ãƒ¡ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°"""
        try:
            # æ—¢å­˜ãƒ¡ãƒƒã‚·ãƒ¥å‰Šé™¤
            for geom in self.mesh_geometries:
                self.vis.remove_geometry(geom, reset_bounding_box=False)
            self.mesh_geometries.clear()
            
            # æ–°ã—ã„ãƒ¡ãƒƒã‚·ãƒ¥ä½œæˆ
            o3d_mesh = o3d.geometry.TriangleMesh()
            o3d_mesh.vertices = o3d.utility.Vector3dVector(event.vertices)
            o3d_mesh.triangles = o3d.utility.Vector3iVector(event.triangles)
            
            # ãƒ¡ãƒƒã‚·ãƒ¥è‰²è¨­å®š
            if event.colors is not None:
                o3d_mesh.vertex_colors = o3d.utility.Vector3dVector(event.colors)
            else:
                o3d_mesh.paint_uniform_color([0.7, 0.7, 0.7])
            
            o3d_mesh.compute_vertex_normals()
            
            # ãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼ã«è¿½åŠ 
            self.vis.add_geometry(o3d_mesh, reset_bounding_box=False)
            self.mesh_geometries.append(o3d_mesh)
            
        except Exception as e:
            print(f"Mesh update from event error: {e}")
    
    def _update_collision_from_event(self, event: CollisionEvent) -> None:
        """è¡çªæ¤œå‡ºã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰è¡çªå¯è¦–åŒ–ã‚’æ›´æ–°"""
        try:
            # æ—¢å­˜è¡çªã‚¸ã‚ªãƒ¡ãƒˆãƒªå‰Šé™¤
            for geom in self.collision_geometries:
                self.vis.remove_geometry(geom, reset_bounding_box=False)
            self.collision_geometries.clear()
            
            # æ–°ã—ã„è¡çªç‚¹ã‚’è¡¨ç¤º
            for collision in event.collision_events:
                sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.02)
                sphere.translate(collision.position)
                sphere.paint_uniform_color([1.0, 0.0, 0.0])  # èµ¤è‰²
                
                self.vis.add_geometry(sphere, reset_bounding_box=False)
                self.collision_geometries.append(sphere)
                
        except Exception as e:
            print(f"Collision update from event error: {e}")
    
    def initialize(self) -> bool:
        """
        ãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼åˆæœŸåŒ–
        
        Returns:
            åˆæœŸåŒ–æˆåŠŸæ™‚True
        """
        try:
            print("çµ±åˆãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼åˆæœŸåŒ–é–‹å§‹...")
            
            # ã‚«ãƒ¡ãƒ©åˆæœŸåŒ–
            if not self.headless_mode:
                self.camera = OrbbecCamera(
                    enable_color=True,
                    depth_width=self.depth_width,
                    depth_height=self.depth_height
                )
                if not self.camera.initialize():
                    print("Failed to initialize camera")
                    return False
                
                # ã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹
                if not self.camera.start():
                    print("Failed to start camera")
                    return False
            
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
            if not self.pipeline.initialize(self.camera):
                print("Failed to initialize pipeline")
                return False
            
            # Open3Dãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼åˆæœŸåŒ–ï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ã§ãªã„å ´åˆï¼‰
            if not self.headless_mode and HAS_OPEN3D:
                if not self._initialize_3d_viewer():
                    print("Warning: 3D viewer initialization failed")
            
            print("çµ±åˆãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼åˆæœŸåŒ–å®Œäº†")
            self.is_initialized = True
            return True
            
        except Exception as e:
            print(f"Integrated viewer initialization error: {e}")
            return False
    
    def _initialize_3d_viewer(self) -> bool:
        """Open3Dãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼åˆæœŸåŒ–"""
        try:
            self.vis = o3d.visualization.Visualizer()
            self.vis.create_window("Geocussion 3D Viewer", width=1280, height=720)
            
            # ç©ºã®ç‚¹ç¾¤ä½œæˆ
            self.pcd = o3d.geometry.PointCloud()
            self.vis.add_geometry(self.pcd)
            
            # ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            render_option = self.vis.get_render_option()
            render_option.point_size = self.point_size
            render_option.background_color = [0.1, 0.1, 0.1]
            render_option.show_coordinate_frame = True
            
            return True
            
        except Exception as e:
            print(f"3D viewer initialization error: {e}")
            return False
    
    def run(self) -> bool:
        """ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ"""
        # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯åˆæœŸåŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if self.headless_mode:
            print("\nğŸ–¥ï¸  ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰: ã‚«ãƒ¡ãƒ©åˆæœŸåŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã¿åˆæœŸåŒ–ï¼ˆã‚«ãƒ¡ãƒ©ãªã—ï¼‰
            if not self.pipeline.initialize(None):
                print("Failed to initialize pipeline for headless mode")
                return False
            self.is_running = True
            self._run_headless_mode()
            return True
        
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆGUIãƒ¢ãƒ¼ãƒ‰ï¼‰ã§ã¯æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã®ã¯ãš
        if not self.is_initialized:
            print("âš ï¸  ãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ - å†åˆæœŸåŒ–ã‚’è©¦è¡Œ")
            if not self.initialize():
                print("Failed to initialize integrated viewer")
                return False
        
        self.is_running = True
        print("\nçµ±åˆGeocussionãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼é–‹å§‹!")
        
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ
        try:
            import cv2
            
            while self.is_running:
                if not self._process_frame():
                    break
                
                # ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å…¥åŠ›ãƒã‚§ãƒƒã‚¯ï¼ˆOpenCVï¼‰
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q') or key == 27:  # 'q' ã¾ãŸã¯ ESC
                    print("çµ‚äº†ã‚­ãƒ¼ãŒæŠ¼ã•ã‚Œã¾ã—ãŸ")
                    break
                elif key != 255:  # ä½•ã‹ã‚­ãƒ¼ãŒæŠ¼ã•ã‚ŒãŸ
                    self._handle_key_event(key)
                
                # Open3Dãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼ã®ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†
                if HAS_OPEN3D and self.vis:
                    self.vis.poll_events()
                    self.vis.update_renderer()
                    
            return True
                    
        except KeyboardInterrupt:
            print("\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
            return True
        except Exception as e:
            print(f"\nãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            self.cleanup()
    
    def _run_headless_mode(self) -> None:
        """ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ"""
        import time
        
        print(f"\nğŸ–¥ï¸  ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ - GUIç„¡åŠ¹åŒ–ã«ã‚ˆã‚‹FPSæœ€é©åŒ–")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {self.headless_duration}ç§’")
        print("=" * 50)
        
        start_time = time.time()
        frame_count = 0
        fps_samples = []
        
        try:
            while True:
                frame_start = time.time()
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ï¼ˆGUIç„¡ã—ï¼‰
                success = self._process_frame_headless()
                
                frame_end = time.time()
                frame_time = frame_end - frame_start
                
                if success:
                    frame_count += 1
                    current_fps = 1.0 / frame_time if frame_time > 0 else 0
                    fps_samples.append(current_fps)
                
                    # 5ç§’é–“éš”ã§çµ±è¨ˆè¡¨ç¤º
                    elapsed = frame_end - start_time
                    if frame_count % 150 == 0:  # ç´„5ç§’é–“éš”
                        avg_fps = sum(fps_samples[-100:]) / len(fps_samples[-100:]) if fps_samples else 0
                        print(f"ğŸ“Š [{elapsed:.1f}s] ãƒ•ãƒ¬ãƒ¼ãƒ : {frame_count}, å¹³å‡FPS: {avg_fps:.1f}, ç¾åœ¨FPS: {current_fps:.1f}")
                
                # å®Ÿè¡Œæ™‚é–“ãƒã‚§ãƒƒã‚¯
                if time.time() - start_time >= self.headless_duration:
                    break
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
        except Exception as e:
            print(f"\nâŒ ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        
        # çµ±è¨ˆè¨ˆç®—
        execution_time = time.time() - start_time
        avg_fps = frame_count / execution_time if execution_time > 0 else 0
        max_fps = max(fps_samples) if fps_samples else 0
        min_fps = min(fps_samples) if fps_samples else 0
        
        # çµæœè¡¨ç¤º
        print("\n" + "=" * 50)
        print("ğŸ ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ å®Ÿè¡Œçµæœ")
        print("=" * 50)
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’")
        print(f"ğŸ¬ ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {frame_count}")
        print(f"ğŸš€ å¹³å‡FPS: {avg_fps:.1f}")
        print(f"ğŸ“ˆ æœ€å¤§FPS: {max_fps:.1f}")
        print(f"ğŸ“‰ æœ€å°FPS: {min_fps:.1f}")
        print()
    
    def _process_frame_headless(self) -> bool:
        """ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹å°‚ç”¨ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ï¼ˆGUIæç”»ãªã—ï¼‰"""
        import time
        try:
            # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ç”¨ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            if not self.camera:
                # ãƒ¢ãƒƒã‚¯æ·±åº¦ãƒ»ã‚«ãƒ©ãƒ¼ç”»åƒç”Ÿæˆ
                import numpy as np
                from ..types import FrameData, CameraIntrinsics
                
                depth_image = np.random.randint(500, 2000, (240, 424), dtype=np.uint16)
                color_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
                
                # ãƒ¢ãƒƒã‚¯Intrinsics
                intrinsics = CameraIntrinsics(
                    fx=209.2152099609375,
                    fy=209.2152099609375,
                    cx=212.3312530517578,
                    cy=119.83750915527344,
                    width=424,
                    height=240
                )
                
                # ãƒ¢ãƒƒã‚¯ãƒ•ãƒ¬ãƒ¼ãƒ ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
                class MockFrame:
                    def __init__(self, data):
                        self.data = data
                    def get_data(self):
                        return self.data
                
                # FrameDataä½œæˆï¼ˆæ­£ã—ã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’ä½¿ç”¨ï¼‰
                frame_data = FrameData(
                    depth_frame=MockFrame(depth_image),
                    color_frame=MockFrame(color_image),
                    timestamp_ms=time.time() * 1000,
                    frame_number=getattr(self, '_mock_frame_number', 0)
                )
                
                # ãƒ¢ãƒƒã‚¯ã‚«ãƒ¡ãƒ©ã®intrinsicsã‚’è¨­å®šï¼ˆInputStageã§ä½¿ç”¨ï¼‰
                if not hasattr(self, '_mock_camera_initialized'):
                    # InputStageã«ãƒ¢ãƒƒã‚¯ã‚«ãƒ¡ãƒ©ã‚’è¨­å®š
                    class MockCamera:
                        def __init__(self, intrinsics):
                            self.depth_intrinsics = intrinsics
                    
                    mock_camera = MockCamera(intrinsics)
                    self.pipeline._orchestrator.input_stage.camera = mock_camera
                    
                    # DetectionStageã«ã‚‚intrinsicsã‚’è¨­å®š
                    self.pipeline._orchestrator.detection_stage.camera_intrinsics = intrinsics
                    self._mock_camera_initialized = True
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ
                self._mock_frame_number = getattr(self, '_mock_frame_number', 0) + 1
                
                # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œï¼ˆãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰
                results = self.pipeline.process_frame(frame_data)
                
                # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯å‡¦ç†é…å»¶ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                time.sleep(0.015)  # 15ms
            else:
                # å®Ÿã‚«ãƒ¡ãƒ©ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—
                results = self.pipeline.process_frame()
            
            if not results:
                return True  # ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—å¤±æ•—ã¯ç¶™ç¶š
            
            self.current_results = results
            
            # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ã§ã¯è¡¨ç¤ºå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã®ã¿æ›´æ–°
            if hasattr(results, 'performance_stats') and results.performance_stats:
                # çµ±è¨ˆæƒ…å ±ã¯å†…éƒ¨ã§ç®¡ç†
                pass
            
            return True
            
        except Exception as e:
            # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ã§ã¯ã‚¨ãƒ©ãƒ¼ã§ã‚‚ç¶™ç¶š
            if not hasattr(self, '_error_count'):
                self._error_count = 0
            self._error_count += 1
            
            if self._error_count <= 3:  # æœ€åˆã®3å›ã®ã¿ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
                print(f"âš ï¸  ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†è­¦å‘Š: {e}")
            return True  # ã‚¨ãƒ©ãƒ¼ã§ã‚‚ç¶™ç¶š
    
    def _process_frame(self) -> bool:
        """ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ï¼ˆé€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼‰"""
        try:
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
            results = self.pipeline.process_frame()
            if not results:
                return True  # ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—å¤±æ•—ã¯ç¶™ç¶š
            
            self.current_results = results
        
            # ã‚«ãƒ©ãƒ¼ç”»åƒæŠ½å‡ºï¼ˆä¿®æ­£ç‰ˆãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼‰
            color_image = self._extract_color_image(results.frame_data if hasattr(results, 'frame_data') and results.frame_data else None)
            
            # å¯è¦–åŒ–æ›´æ–°
            self._update_visualization(results, color_image)
            
            return True
        
        except Exception as e:
            print(f"Frame processing error: {e}")
            return False
        
    def _update_visualization(self, results, color_image) -> None:
        """å¯è¦–åŒ–æ›´æ–°"""
        try:
            # çµæœã‚’ä¿å­˜
            self.current_results = results
            
            # RGBè¡¨ç¤ºå‡¦ç†
            if color_image is not None:
                self._process_rgb_display_with_image(color_image)
            
            # ç‚¹ç¾¤è¡¨ç¤ºå‡¦ç†
            if HAS_OPEN3D and self.vis:
                self._process_pointcloud_display()
                
        except Exception as e:
            print(f"Visualization update error: {e}")
    
    def _process_rgb_display_with_image(self, color_image) -> None:
        """RGBè¡¨ç¤ºå‡¦ç†ï¼ˆç”»åƒä»˜ãï¼‰"""
        try:
            if color_image is None:
                return
            
            import cv2
            
            # æ‰‹æ¤œå‡ºçµæœã®æç”»
            if hasattr(self.current_results, 'hands_2d') and self.current_results.hands_2d:
                for hand in self.current_results.hands_2d:
                    if hasattr(hand, 'landmarks'):
                        for landmark in hand.landmarks:
                            x = int(landmark.x * color_image.shape[1])
                            y = int(landmark.y * color_image.shape[0])
                            cv2.circle(color_image, (x, y), 3, (0, 255, 0), -1)
            
            # è¡çªçµæœã®æç”»
            if hasattr(self.current_results, 'collision_events') and self.current_results.collision_events:
                cv2.putText(color_image, f"Collisions: {len(self.current_results.collision_events)}", 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            
            # ç”»åƒè¡¨ç¤º
            cv2.imshow('Geocussion-SP Color', color_image)
            cv2.waitKey(1)
            
        except Exception as e:
            print(f"RGB display error: {e}")
    
    def _process_pointcloud_display(self) -> bool:
        """ç‚¹ç¾¤è¡¨ç¤ºå‡¦ç†"""
        if not self.current_results:
            return True
        
        # ãƒ¡ãƒƒã‚·ãƒ¥å¯è¦–åŒ–æ›´æ–°
        if self.current_results.mesh:
            self._update_mesh_visualization(self.current_results.mesh)
        
        # è¡çªå¯è¦–åŒ–æ›´æ–°
        if self.pipeline_config.enable_collision_visualization:
            self._update_collision_visualization()
        
        # æ‰‹ãƒãƒ¼ã‚«ãƒ¼æ›´æ–°
        self._update_hand_markers()
        
        # Open3Dãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼æ›´æ–°
        if self.vis:
            self.vis.poll_events()
            self.vis.update_renderer()
        
        return True
    
    def _draw_hand_detections(self, image: np.ndarray) -> None:
        """æ‰‹æ¤œå‡ºçµæœæç”»"""
        height, width = image.shape[:2]
        
        # 2Dæ‰‹æ¤œå‡ºçµæœæç”»
        for hand_2d in self.current_results.hands_2d:
            # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹
            bbox = hand_2d.bounding_box
            scale_x = width / 640
            scale_y = height / 480
            
            bbox_scaled = (
                int(bbox[0] * scale_x),
                int(bbox[1] * scale_y),
                int(bbox[2] * scale_x),
                int(bbox[3] * scale_y)
            )
            
            cv2.rectangle(image, 
                         (bbox_scaled[0], bbox_scaled[1]), 
                         (bbox_scaled[0] + bbox_scaled[2], bbox_scaled[1] + bbox_scaled[3]), 
                         (0, 255, 255), 2)
            
            # æ‰‹ã®æƒ…å ±
            cv2.putText(image, f"{hand_2d.handedness.value} ({hand_2d.confidence:.2f})",
                       (bbox_scaled[0], bbox_scaled[1] - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        
        # 3Dæƒ…å ±è¡¨ç¤º
        info_y = 60
        if self.current_results.hands_3d:
            cv2.putText(image, f"3D Hands: {len(self.current_results.hands_3d)}", 
                       (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
        
        # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°æƒ…å ±è¡¨ç¤º
        if self.current_results.tracked_hands:
            track_y = info_y + 50
            cv2.putText(image, f"Tracked: {len(self.current_results.tracked_hands)}", 
                       (10, track_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    
    def _draw_collision_info(self, image: np.ndarray) -> None:
        """è¡çªæƒ…å ±æç”»"""
        y_offset = 30
        for i, event in enumerate(self.current_results.collision_events):
            text = f"Collision {i+1}: Hand {event.get('hand_id', 'Unknown')[:8]}"
            cv2.putText(image, text, (10, y_offset), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            y_offset += 25
    
    def _draw_performance_overlay(self, image: np.ndarray) -> None:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±æç”»"""
        stats = self.current_results.performance_stats
        y_offset = image.shape[0] - 150
        
        texts = [
            f"Frame: {stats.get('frame_count', 0)}",
            f"Mesh: {stats.get('mesh_generation_time', 0)*1000:.1f}ms",
            f"Collision: {stats.get('collision_detection_time', 0)*1000:.1f}ms",
            f"Audio: {stats.get('audio_synthesis_time', 0)*1000:.1f}ms",
            f"Total: {stats.get('total_pipeline_time', 0)*1000:.1f}ms",
            f"Events: {stats.get('collision_events_count', 0)}",
            f"Notes: {stats.get('audio_notes_played', 0)}"
        ]
        
        for text in texts:
            cv2.putText(image, text, (10, y_offset), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            y_offset += 20
    
    def _update_mesh_visualization(self, mesh) -> None:
        """ãƒ¡ãƒƒã‚·ãƒ¥å¯è¦–åŒ–æ›´æ–°"""
        try:
            # æ—¢å­˜ãƒ¡ãƒƒã‚·ãƒ¥å‰Šé™¤
            for geom in self.mesh_geometries:
                self.vis.remove_geometry(geom, reset_bounding_box=False)
            self.mesh_geometries.clear()
            
            if mesh and hasattr(mesh, 'vertices') and len(mesh.vertices) > 0:
                # Open3Dãƒ¡ãƒƒã‚·ãƒ¥ä½œæˆ
                o3d_mesh = o3d.geometry.TriangleMesh()
                o3d_mesh.vertices = o3d.utility.Vector3dVector(mesh.vertices)
                if hasattr(mesh, 'triangles') and len(mesh.triangles) > 0:
                    o3d_mesh.triangles = o3d.utility.Vector3iVector(mesh.triangles)
                
                # ãƒ¡ãƒƒã‚·ãƒ¥è‰²è¨­å®š
                o3d_mesh.paint_uniform_color([0.7, 0.7, 0.7])
                o3d_mesh.compute_vertex_normals()
                
                # ãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼ã«è¿½åŠ 
                self.vis.add_geometry(o3d_mesh, reset_bounding_box=False)
                self.mesh_geometries.append(o3d_mesh)
                
        except Exception as e:
            print(f"Mesh visualization error: {e}")
    
    def _update_collision_visualization(self) -> None:
        """è¡çªå¯è¦–åŒ–æ›´æ–°"""
        try:
            # æ—¢å­˜è¡çªã‚¸ã‚ªãƒ¡ãƒˆãƒªå‰Šé™¤
            for geom in self.collision_geometries:
                self.vis.remove_geometry(geom, reset_bounding_box=False)
            self.collision_geometries.clear()
            
            if self.current_results and self.current_results.collision_events:
                for event in self.current_results.collision_events:
                    if 'position' in event:
                        # è¡çªç‚¹ã«çƒè¡¨ç¤º
                        sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.02)
                        sphere.translate(event['position'])
                        sphere.paint_uniform_color([1.0, 0.0, 0.0])  # èµ¤è‰²
                        
                        self.vis.add_geometry(sphere, reset_bounding_box=False)
                        self.collision_geometries.append(sphere)
                        
        except Exception as e:
            print(f"Collision visualization error: {e}")
    
    def _update_hand_markers(self) -> None:
        """æ‰‹ãƒãƒ¼ã‚«ãƒ¼æ›´æ–°"""
        try:
            # æ—¢å­˜æ‰‹ãƒãƒ¼ã‚«ãƒ¼å‰Šé™¤
            for marker in self.hand_markers:
                self.vis.remove_geometry(marker, reset_bounding_box=False)
            self.hand_markers.clear()
            
            if self.current_results and self.current_results.tracked_hands:
                for hand in self.current_results.tracked_hands:
                    if hasattr(hand, 'palm_center') and hand.palm_center is not None:
                        # æ‰‹ã®ä¸­å¿ƒã«çƒè¡¨ç¤º
                        sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.03)
                        sphere.translate(hand.palm_center)
                        sphere.paint_uniform_color([0.0, 1.0, 0.0])  # ç·‘è‰²
                        
                        self.vis.add_geometry(sphere, reset_bounding_box=False)
                        self.hand_markers.append(sphere)
                        
        except Exception as e:
            print(f"Hand marker update error: {e}")
    
    def _handle_key_event(self, key: int) -> None:
        """ã‚­ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†"""
        # ã‚­ãƒ¼æŠ¼ä¸‹ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºè¡Œ
        self.event_dispatcher.publish(KeyPressedEvent(
            key_code=key,
            shift=False,  # TODO: ä¿®é£¾ã‚­ãƒ¼ã®æ¤œå‡º
            ctrl=False,
            alt=False
        ))
        if key == ord('h') or key == ord('H'):
            print(self.help_text)
        
        elif key == ord('m') or key == ord('M'):
            self.pipeline_config.enable_mesh_generation = not self.pipeline_config.enable_mesh_generation
            self.pipeline.update_config(self.pipeline_config)
            status = "æœ‰åŠ¹" if self.pipeline_config.enable_mesh_generation else "ç„¡åŠ¹"
            print(f"ãƒ¡ãƒƒã‚·ãƒ¥ç”Ÿæˆ: {status}")
        
        elif key == ord('c') or key == ord('C'):
            self.pipeline_config.enable_collision_detection = not self.pipeline_config.enable_collision_detection
            self.pipeline.update_config(self.pipeline_config)
            status = "æœ‰åŠ¹" if self.pipeline_config.enable_collision_detection else "ç„¡åŠ¹"
            print(f"è¡çªæ¤œå‡º: {status}")
        
        elif key == ord('v') or key == ord('V'):
            self.pipeline_config.enable_collision_visualization = not self.pipeline_config.enable_collision_visualization
            status = "æœ‰åŠ¹" if self.pipeline_config.enable_collision_visualization else "ç„¡åŠ¹"
            print(f"è¡çªå¯è¦–åŒ–: {status}")
        
        elif key == ord('n') or key == ord('N'):
            print("ãƒ¡ãƒƒã‚·ãƒ¥å¼·åˆ¶æ›´æ–°...")
            self.pipeline.force_mesh_update()
        
        elif key == ord('+') or key == ord('='):
            self.pipeline_config.sphere_radius = min(self.pipeline_config.sphere_radius + 0.01, 0.2)
            self.pipeline.update_config(self.pipeline_config)
            print(f"çƒåŠå¾„: {self.pipeline_config.sphere_radius*100:.1f}cm")
        
        elif key == ord('-') or key == ord('_'):
            self.pipeline_config.sphere_radius = max(self.pipeline_config.sphere_radius - 0.01, 0.01)
            self.pipeline.update_config(self.pipeline_config)
            print(f"çƒåŠå¾„: {self.pipeline_config.sphere_radius*100:.1f}cm")
        
        elif key == ord('p') or key == ord('P'):
            self.show_performance = not self.show_performance
            status = "è¡¨ç¤º" if self.show_performance else "éè¡¨ç¤º"
            print(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ: {status}")
        
        elif key == ord('a') or key == ord('A'):
            self.pipeline_config.enable_audio_synthesis = not self.pipeline_config.enable_audio_synthesis
            self.pipeline.update_config(self.pipeline_config)
            status = "æœ‰åŠ¹" if self.pipeline_config.enable_audio_synthesis else "ç„¡åŠ¹"
            print(f"éŸ³éŸ¿åˆæˆ: {status}")
        
        elif key == ord('s') or key == ord('S'):
            if self.pipeline_config.enable_audio_synthesis:
                self._cycle_audio_scale()
        
        elif key == ord('i') or key == ord('I'):
            if self.pipeline_config.enable_audio_synthesis:
                self._cycle_audio_instrument()
        
        elif key == ord('1'):
            self.pipeline_config.audio_master_volume = max(0.0, self.pipeline_config.audio_master_volume - 0.1)
            self.pipeline.update_config(self.pipeline_config)
            print(f"éŸ³é‡: {self.pipeline_config.audio_master_volume:.1f}")
        
        elif key == ord('2'):
            self.pipeline_config.audio_master_volume = min(1.0, self.pipeline_config.audio_master_volume + 0.1)
            self.pipeline.update_config(self.pipeline_config)
            print(f"éŸ³é‡: {self.pipeline_config.audio_master_volume:.1f}")
    
    def _cycle_audio_scale(self) -> None:
        """éŸ³éšåˆ‡ã‚Šæ›¿ãˆ"""
        scales = list(ScaleType)
        current_index = scales.index(self.pipeline_config.audio_scale)
        next_index = (current_index + 1) % len(scales)
        self.pipeline_config.audio_scale = scales[next_index]
        self.pipeline.update_config({'audio_scale': self.pipeline_config.audio_scale})
        print(f"éŸ³éš: {self.pipeline_config.audio_scale.value}")
    
    def _cycle_audio_instrument(self) -> None:
        """æ¥½å™¨åˆ‡ã‚Šæ›¿ãˆ"""
        instruments = list(InstrumentType)
        current_index = instruments.index(self.pipeline_config.audio_instrument)
        next_index = (current_index + 1) % len(instruments)
        self.pipeline_config.audio_instrument = instruments[next_index]
        self.pipeline.update_config({'audio_instrument': self.pipeline_config.audio_instrument})
        print(f"æ¥½å™¨: {self.pipeline_config.audio_instrument.value}")
    
    def _extract_color_image(self, frame_data):
        """ã‚«ãƒ©ãƒ¼ç”»åƒæŠ½å‡ºï¼ˆMJPGå¯¾å¿œãƒ»C-contiguouså®Œå…¨å¯¾å¿œç‰ˆï¼‰"""
        try:
            if frame_data is None:
                return self._create_fallback_image()
                
            if not hasattr(frame_data, 'color_frame') or frame_data.color_frame is None:
                return self._create_fallback_image()
                
            if not hasattr(self.camera, 'has_color') or not self.camera.has_color:
                return self._create_fallback_image()
            
            import cv2
            from ..types import OBFormat
            
            try:
                color_frame = frame_data.color_frame
                frame_format = getattr(color_frame, 'get_format', lambda: OBFormat.RGB)()
                

                
                color_image = None
                
                if str(frame_format) == "OBFormat.MJPG" or str(frame_format) == "MJPG":
                    # MJPGå½¢å¼ã®å ´åˆï¼šJPEGãƒ‡ã‚³ãƒ¼ãƒ‰ãŒå¿…è¦
                    try:
                        # get_data()ã§JPEGãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                        jpeg_data = color_frame.get_data()
                        if jpeg_data is None:
                            if not hasattr(self, '_mjpg_viewer_no_data_error_shown'):
                                print("MJPG viewer: No data from color frame")
                                self._mjpg_viewer_no_data_error_shown = True
                            return self._create_fallback_image()
                        
                        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’ç¢ºèª
                        data_size = 0
                        jpeg_bytes = None
                        
                        # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’numpyé…åˆ—ã«å¤‰æ›ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã§è©¦è¡Œï¼‰
                        try:
                            if hasattr(jpeg_data, 'tobytes'):
                                jpeg_bytes = jpeg_data.tobytes()
                            elif hasattr(jpeg_data, '__bytes__'):
                                jpeg_bytes = bytes(jpeg_data)
                            elif hasattr(jpeg_data, '__array__'):
                                jpeg_array_raw = np.array(jpeg_data, dtype=np.uint8)
                                jpeg_bytes = jpeg_array_raw.tobytes()
                            else:
                                # æœ€å¾Œã®æ‰‹æ®µï¼šç›´æ¥bytes()ã‚’è©¦è¡Œ
                                jpeg_bytes = bytes(jpeg_data)
                            
                            data_size = len(jpeg_bytes)
                            
                        except Exception as data_convert_error:
                            if not hasattr(self, '_mjpg_viewer_data_convert_error_shown'):
                                print(f"MJPG viewer data conversion failed: {data_convert_error}")
                                self._mjpg_viewer_data_convert_error_shown = True
                            return self._create_fallback_image()
                        
                        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¦¥å½“ã‹ãƒã‚§ãƒƒã‚¯
                        if data_size < 100:  # JPEGã¯æœ€ä½ã§ã‚‚100ãƒã‚¤ãƒˆä»¥ä¸Šå¿…è¦
                            if not hasattr(self, '_mjpg_viewer_small_data_error_shown'):
                                print(f"MJPG viewer data too small: {data_size} bytes")
                                self._mjpg_viewer_small_data_error_shown = True
                            return self._create_fallback_image()
                        
                        # OpenCVã§JPEGãƒ‡ã‚³ãƒ¼ãƒ‰
                        jpeg_array = np.frombuffer(jpeg_bytes, dtype=np.uint8)
                        bgr_image = cv2.imdecode(jpeg_array, cv2.IMREAD_COLOR)
                        
                        if bgr_image is None:
                            if not hasattr(self, '_mjpg_viewer_opencv_decode_error_shown'):
                                print(f"OpenCV MJPG decode failed in viewer (data size: {data_size} bytes)")
                                self._mjpg_viewer_opencv_decode_error_shown = True
                            return self._create_fallback_image()
                        
                        # ç”»åƒã‚µã‚¤ã‚ºã‚’ç¢ºèª
                        if bgr_image.shape[0] == 0 or bgr_image.shape[1] == 0:
                            if not hasattr(self, '_mjpg_viewer_zero_size_error_shown'):
                                print(f"MJPG viewer decoded to zero size image: {bgr_image.shape}")
                                self._mjpg_viewer_zero_size_error_shown = True
                            return self._create_fallback_image()
                        
                        # BGRã‹ã‚‰RGBã«å¤‰æ›
                        color_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)
                        # C-contiguousé…åˆ—ã¨ã—ã¦ç¢ºå®Ÿã«ä½œæˆ
                        color_image = np.ascontiguousarray(color_image, dtype=np.uint8)
                        
                        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åˆå›ã®ã¿è¡¨ç¤º
                        if not hasattr(self, '_mjpg_viewer_success_shown'):
                            print(f"âœ… MJPG viewer processing successful: {color_image.shape} ({data_size} bytes)")
                            self._mjpg_viewer_success_shown = True
                        
                    except Exception as e:
                        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åˆå›ã®ã¿è¡¨ç¤º
                        if not hasattr(self, '_mjpg_viewer_error_shown'):
                            print(f"MJPG processing error in viewer: {e}")
                            self._mjpg_viewer_error_shown = True
                        return self._create_fallback_image()
                
                elif str(frame_format) in ["OBFormat.RGB", "RGB"]:
                    # RGBå½¢å¼ã®å ´åˆ
                    try:
                        raw_data = color_frame.get_data()
                        if raw_data is None:
                            return self._create_fallback_image()
                        
                        # ã‚«ãƒ¡ãƒ©ã®è§£åƒåº¦æƒ…å ±ã‚’å–å¾—
                        width = getattr(color_frame, 'get_width', lambda: 1280)()
                        height = getattr(color_frame, 'get_height', lambda: 720)()
                        
                        # numpyé…åˆ—ã«å¤‰æ›
                        if hasattr(raw_data, 'tobytes'):
                            data_bytes = raw_data.tobytes()
                        else:
                            data_bytes = bytes(raw_data)
                        
                        color_array = np.frombuffer(data_bytes, dtype=np.uint8)
                        color_image = color_array.reshape((height, width, 3))
                        # C-contiguousé…åˆ—ã¨ã—ã¦ç¢ºå®Ÿã«ä½œæˆ
                        color_image = np.ascontiguousarray(color_image, dtype=np.uint8)
                        
                    except Exception as e:
                        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åˆå›ã®ã¿è¡¨ç¤º
                        if not hasattr(self, '_rgb_viewer_error_shown'):
                            print(f"RGB processing error in viewer: {e}")
                            self._rgb_viewer_error_shown = True
                        return self._create_fallback_image()
                
                elif str(frame_format) in ["OBFormat.BGR", "BGR"]:
                    # BGRå½¢å¼ã®å ´åˆ
                    try:
                        raw_data = color_frame.get_data()
                        if raw_data is None:
                            return self._create_fallback_image()
                        
                        # ã‚«ãƒ¡ãƒ©ã®è§£åƒåº¦æƒ…å ±ã‚’å–å¾—
                        width = getattr(color_frame, 'get_width', lambda: 1280)()
                        height = getattr(color_frame, 'get_height', lambda: 720)()
                        
                        # numpyé…åˆ—ã«å¤‰æ›
                        if hasattr(raw_data, 'tobytes'):
                            data_bytes = raw_data.tobytes()
                        else:
                            data_bytes = bytes(raw_data)
                        
                        color_array = np.frombuffer(data_bytes, dtype=np.uint8)
                        color_image = color_array.reshape((height, width, 3))
                        # C-contiguousé…åˆ—ã¨ã—ã¦ç¢ºå®Ÿã«ä½œæˆ
                        color_image = np.ascontiguousarray(color_image, dtype=np.uint8)
                        
                        # BGRâ†’RGBå¤‰æ›
                        color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
                        color_image = np.ascontiguousarray(color_image, dtype=np.uint8)
                        
                    except Exception as e:
                        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åˆå›ã®ã¿è¡¨ç¤º
                        if not hasattr(self, '_bgr_viewer_error_shown'):
                            print(f"BGR processing error in viewer: {e}")
                            self._bgr_viewer_error_shown = True
                        return self._create_fallback_image()
                
                else:
                    # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å½¢å¼ï¼ˆåˆå›ã®ã¿è¡¨ç¤ºï¼‰
                    if not hasattr(self, '_unsupported_format_viewer_error_shown'):
                        print(f"Unsupported color format in viewer: {frame_format}")
                        self._unsupported_format_viewer_error_shown = True
                    return self._create_fallback_image()
                
                # è¡¨ç¤ºç”¨ã«ãƒªã‚µã‚¤ã‚ºï¼ˆC-contiguousç¶­æŒï¼‰
                if color_image is not None:
                    # C-contiguousã‚’ç¢ºèªã—ã¦ã‹ã‚‰ãƒªã‚µã‚¤ã‚º
                    if not color_image.flags['C_CONTIGUOUS']:
                        color_image = np.ascontiguousarray(color_image)
                    
                    color_resized = cv2.resize(color_image, self.rgb_window_size)
                    # ãƒªã‚µã‚¤ã‚ºå¾Œã‚‚C-contiguousã«ã™ã‚‹
                    return np.ascontiguousarray(color_resized, dtype=np.uint8)
                
                return self._create_fallback_image()
                
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åˆå›ã®ã¿è¡¨ç¤º
                if not hasattr(self, '_color_extraction_viewer_error_shown'):
                    print(f"Color frame access error in viewer: {e}")
                    self._color_extraction_viewer_error_shown = True
                return self._create_fallback_image()
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åˆå›ã®ã¿è¡¨ç¤º
            if not hasattr(self, '_color_image_general_error_shown'):
                print(f"Color image extraction general error: {e}")
                self._color_image_general_error_shown = True
            return self._create_fallback_image()
    
    def _create_fallback_image(self):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®é»’ç”»åƒã‚’ä½œæˆï¼ˆC-contiguousä¿è¨¼ï¼‰"""
        try:
            fallback_image = np.zeros((self.rgb_window_size[1], self.rgb_window_size[0], 3), dtype=np.uint8)
            return np.ascontiguousarray(fallback_image)
        except Exception:
            # æœ€æ‚ªã®å ´åˆã€å°ã•ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”»åƒ
            return np.ascontiguousarray(np.zeros((240, 320, 3), dtype=np.uint8))
    
    def cleanup(self) -> None:
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†"""
        try:
            self.is_running = False
            self.is_initialized = False
            
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.pipeline:
                self.pipeline.cleanup()
            
            # Open3Dãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.vis:
                self.vis.destroy_window()
            
            # OpenCVã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cv2.destroyAllWindows()
            
            print("çµ±åˆãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
            
        except Exception as e:
            print(f"Cleanup error: {e}")